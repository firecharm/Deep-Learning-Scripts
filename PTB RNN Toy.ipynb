{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a reproduciton & interpretation of http://adventuresinmachinelearning.com/keras-lstm-tutorial/ and https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras_lstm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> n years o\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"/Users/yaoyucui/Works/Git/PTB-dataset-from-Tomas-Mikolov-s-webpage-master/data/\"\n",
    "\n",
    "train_text = open(os.path.join(path+\"ptb.train.txt\")).read().lower()\n",
    "test_text = open(os.path.join(path+\"ptb.test.txt\")).read().lower()\n",
    "valid_text = open(os.path.join(path+\"ptb.valid.txt\")).read().lower()\n",
    "\n",
    "print(train_text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter  pierre  n years old wil\n"
     ]
    }
   ],
   "source": [
    "train_text=train_text.replace(\"\\n\",\"\")\n",
    "train_text=train_text.replace(\"<unk>\",\"\")\n",
    "test_text=test_text.replace(\"\\n\",\"\")\n",
    "test_text=test_text.replace(\"<unk>\",\"\")\n",
    "valid_text=valid_text.replace(\"\\n\",\"\")\n",
    "valid_text=valid_text.replace(\"<unk>\",\"\")\n",
    "   \n",
    "print(train_text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "train_list = text_to_word_sequence(train_text)\n",
    "test_list = text_to_word_sequence(test_text)\n",
    "valid_list = text_to_word_sequence(valid_text)\n",
    "\n",
    "train_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter(train_list)\n",
    "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "words, _ = list(zip(*count_pairs))\n",
    "word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "train_data = [word_to_id[word] for word in train_list if word in word_to_id]\n",
    "test_data = [word_to_id[word] for word in test_list if word in word_to_id]\n",
    "valid_data = [word_to_id[word] for word in valid_list if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9619, 9620, 9621, 9623, 9624, 9625, 9628, 9629, 9630, 9631]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "Model first use one batch of (batch size 20 x step size 5 = 100 words) to predict followed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code expalin:\n",
    "# default skiptep set to 5, for the first iteration,\n",
    "# x (in a shape of 20,5) and y (in a shape of 20,5,vocab_size) will be replaced to :\n",
    "#    x[1,:] 1st word, 2nd word, 3rd word, 4th word, 5th word   y is a one-hot-encoding for 6th - 10th word\n",
    "# Hence, using the first 5 word to predict next five word.\n",
    "# And continue from the 6th onward (since skip_num = 5)\n",
    "\n",
    "batch_size = 20\n",
    "num_steps = 5\n",
    "current_idx = 0\n",
    "skip_step = 5\n",
    "vocab_size = len(word_to_id)\n",
    "x = np.zeros((batch_size, num_steps))\n",
    "y = np.zeros((batch_size, num_steps, vocab_size))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x[i, :] = train_data[current_idx:current_idx + num_steps]\n",
    "    temp_y = train_data[current_idx + 1:current_idx + num_steps + 1]\n",
    "    y[i, :, :] = to_categorical(temp_y, num_classes=vocab_size)\n",
    "    current_idx += skip_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.619e+03, 9.620e+03, 9.621e+03, 9.623e+03, 9.624e+03],\n",
       "       [9.625e+03, 9.628e+03, 9.629e+03, 9.630e+03, 9.631e+03],\n",
       "       [5.126e+03, 9.632e+03, 9.634e+03, 9.635e+03, 9.636e+03],\n",
       "       [9.637e+03, 9.639e+03, 9.640e+03, 9.641e+03, 9.642e+03],\n",
       "       [9.643e+03, 9.644e+03, 5.030e+02, 9.645e+03, 9.646e+03],\n",
       "       [9.647e+03, 8.939e+03, 1.000e+00, 7.300e+01, 3.560e+02],\n",
       "       [3.000e+01, 2.126e+03, 0.000e+00, 1.460e+02, 1.600e+01],\n",
       "       [4.000e+00, 8.891e+03, 2.840e+02, 4.180e+02, 1.000e+00],\n",
       "       [2.000e+01, 1.000e+01, 1.440e+02, 2.000e+00, 1.000e+00],\n",
       "       [2.501e+03, 0.000e+00, 3.063e+03, 1.600e+03, 9.400e+01],\n",
       "       [7.461e+03, 1.000e+00, 7.300e+01, 3.560e+02, 6.000e+00],\n",
       "       [3.410e+02, 1.440e+02, 2.000e+00, 2.461e+03, 6.700e+02],\n",
       "       [2.160e+03, 9.650e+02, 2.100e+01, 5.310e+02, 4.000e+00],\n",
       "       [8.891e+03, 2.840e+02, 2.000e+00, 3.700e+01, 3.090e+02],\n",
       "       [4.500e+02, 3.643e+03, 4.000e+00, 9.510e+02, 2.000e+00],\n",
       "       [3.129e+03, 5.080e+02, 2.710e+02, 3.000e+00, 1.410e+02],\n",
       "       [5.966e+03, 4.190e+03, 5.911e+03, 2.700e+01, 9.950e+02],\n",
       "       [4.000e+00, 1.640e+02, 7.760e+02, 2.000e+00, 9.500e+02],\n",
       "       [2.763e+03, 2.200e+02, 4.000e+00, 9.400e+01, 2.000e+00],\n",
       "       [4.410e+02, 4.064e+03, 3.000e+00, 1.100e+01, 4.400e+01]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9619,\n",
       " 9620,\n",
       " 9621,\n",
       " 9623,\n",
       " 9624,\n",
       " 9625,\n",
       " 9628,\n",
       " 9629,\n",
       " 9630,\n",
       " 9631,\n",
       " 5126,\n",
       " 9632,\n",
       " 9634,\n",
       " 9635,\n",
       " 9636,\n",
       " 9637,\n",
       " 9639,\n",
       " 9640,\n",
       " 9641,\n",
       " 9642]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)\n",
    "num_steps = 30\n",
    "batch_size = 200\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary = vocab_size,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary = vocab_size,\n",
    "                                           skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "\n",
    "Please excuse the initial parameters, I am using a Mac :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 30, 30)            289440    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 30, 30)            7320      \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 30, 30)            7320      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30, 30)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 9648)          299088    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 30, 9648)          0         \n",
      "=================================================================\n",
      "Total params: 603,168\n",
      "Trainable params: 603,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,TimeDistributed,Dropout,Dense,Activation\n",
    "from keras import optimizers\n",
    "hidden_size = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "140/140 [==============================] - 446s 3s/step - loss: 7.8163 - categorical_accuracy: 0.0440\n",
      "Epoch 2/5\n",
      "140/140 [==============================] - 425s 3s/step - loss: 6.9377 - categorical_accuracy: 0.0604\n",
      "Epoch 3/5\n",
      "140/140 [==============================] - 412s 3s/step - loss: 6.9077 - categorical_accuracy: 0.0603\n",
      "Epoch 4/5\n",
      "140/140 [==============================] - 437s 3s/step - loss: 6.8953 - categorical_accuracy: 0.0604\n",
      "Epoch 5/5\n",
      "140/140 [==============================] - 416s 3s/step - loss: 6.8898 - categorical_accuracy: 0.0604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1340b55f8>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "num_epochs = 5\n",
    "# checkpointer = ModelCheckpoint(filepath=\"/Users/yaoyucui/Works/Git/PTB-dataset-from-Tomas-Mikolov-s-webpage-master/data\" + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "# model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "#                         validation_data=valid_data_generator.generate(),\n",
    "#                         validation_steps=len(valid_data)//(batch_size*num_steps),callbacks=[checkpointer])\n",
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps),num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result is a lot worse than I expected, this could be caused by:\n",
    "\n",
    "1. ptb dataset is widly used for ML testing of NLP, there are lots of noise in it.\n",
    "\n",
    "2. In the original script, words were sorted as its place on a frequency list, there are surely better ways for embedding, this is merely a toy script.\n",
    "\n",
    "3. hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
